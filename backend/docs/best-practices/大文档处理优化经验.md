# å¤§æ–‡æ¡£å¤„ç†ä¼˜åŒ–ç»éªŒ - è·¨é¡¹ç›®å¤ç”¨æŒ‡å—

> **ç‰ˆæœ¬**: 1.0
> **é€‚ç”¨åœºæ™¯**: LLM Agentå¤„ç†è¶…é•¿æ–‡æ¡£/å¤§è§„æ¨¡ææ–™é›†çš„ä¼˜åŒ–
> **æ ¸å¿ƒä»·å€¼**: æŠ€æœ¯ä¼˜åŒ– + ä¸šåŠ¡é€»è¾‘ = æ™ºèƒ½åŒ–å¤„ç†

---

## ğŸ“‹ é—®é¢˜è¯†åˆ«æ¸…å•

é‡åˆ°ä»¥ä¸‹ä»»ä¸€é—®é¢˜ï¼Œè€ƒè™‘åº”ç”¨æœ¬æ–‡æ¡£æ–¹æ¡ˆï¼š

- [ ] Agentéœ€è¦äºŒæ¬¡è°ƒç”¨æ‰èƒ½å®Œæˆä»»åŠ¡ï¼ˆåˆæ­¥æå– â†’ è¯¦ç»†æå–ï¼‰
- [ ] å•ä¸ªæ–‡æ¡£>50KBå¯¼è‡´tokenæº¢å‡ºæˆ–æå–å¤±è´¥
- [ ] å¤šä¸ªæ–‡æ¡£ä¸²è¡Œå¤„ç†æ•ˆç‡ä½ï¼ˆ10+æ–‡æ¡£ï¼‰
- [ ] ç¼ºå°‘å¤„ç†è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯è¿½æº¯æ€§

---

## ğŸ¯ ä¸‰å±‚ä¼˜åŒ–æ¡†æ¶

### ç¬¬ä¸€å±‚ï¼šå¼ºåˆ¶å®Œæ•´æ€§ï¼ˆè§£å†³äºŒæ¬¡è°ƒç”¨ï¼‰

**ç—‡çŠ¶**: Agentåªå®Œæˆ"åˆæ­¥å·¥ä½œ"å°±åœæ­¢ï¼Œéœ€è¦ç”¨æˆ·å†æ¬¡è°ƒç”¨

**è§£å†³æ–¹æ¡ˆ**:
```markdown
## âš ï¸ MANDATORY: Full Workflow Completion Commitment

**CRITICAL**: You MUST complete ALL N steps in this single invocation.

### What "Complete" Means:
âœ“ ALL items identified in Step X MUST have detailed processing in Step Y
âœ“ NO items should be marked as "[å¾…å¤„ç†]" or "pending"

### Prohibited Actions:
âŒ DO NOT stop after initial steps expecting second invocation
âŒ DO NOT output partial results

### If You Encounter Technical Limitations:
1. Process as many as possible using all strategies
2. Output detailed progress report explaining barriers
3. Mark incomplete status clearly in output
```

**å®æ–½ä½ç½®**: åœ¨å·¥ä½œæµç¨‹å¼€å§‹å‰ï¼ˆStep 0ä¹‹åï¼‰

**æ•ˆæœ**: äºŒæ¬¡è°ƒç”¨éœ€æ±‚ä»90% â†’ <10%

---

### ç¬¬äºŒå±‚ï¼šæ–‡æ¡£åˆ†æ®µç­–ç•¥ï¼ˆè§£å†³è¶…é•¿æ–‡æ¡£ï¼‰

**ç—‡çŠ¶**: å•ä¸ªæ–‡æ¡£>50KBæˆ–>1500è¡Œï¼Œå¯¼è‡´è¯»å–å¤±è´¥

**è§£å†³æ–¹æ¡ˆA: Section-Based Chunking**ï¼ˆé€‚åˆï¼šå•ä¸ªè¶…é•¿æ–‡æ¡£ï¼‰

```python
# 1. æ–‡ä»¶å¤§å°é¢„æ£€æµ‹
if file_size_kb > 50:
    strategy = "chunked"

# 2. æ–‡æ¡£ç»“æ„å¿«é€Ÿæ‰«æï¼ˆå‰200è¡Œï¼‰
sections = scan_structure(file, limit=200)
# è¯†åˆ«ï¼šç« ç¨‹ã€åè®®ã€å†³è®®ç­‰ç« èŠ‚ä½ç½®

# 3. æŒ‰éœ€è¯»å–ï¼ˆåªè¯»ç›¸å…³ç« èŠ‚ï¼‰
EVENT_SECTION_MAP = {
    'è‚¡æƒè½¬è®©': ['è½¬è®©åè®®', 'è‚¡ä¸œä¼šå†³è®®'],
    'å¢èµ„': ['å¢èµ„åè®®', 'ç« ç¨‹'],
}
sections_needed = EVENT_SECTION_MAP[event_type]
for section in sections_needed:
    content = Read(file, offset=section_start, limit=400)
```

**å…³é”®å‚æ•°**:
- æ£€æµ‹é˜ˆå€¼: 50KB æˆ– 1500è¡Œ
- æ‰«æè¡Œæ•°: 200è¡Œ
- åˆ†æ®µå¤§å°: 300-500è¡Œï¼ˆè§†ç« èŠ‚ç±»å‹ï¼‰
- é‡å åŒº: 30-50è¡Œï¼ˆé˜²æ­¢ä¿¡æ¯é—æ¼ï¼‰

**æ•ˆæœ**: Tokenä½¿ç”¨â†“60-70%ï¼Œæ”¯æŒ150KB+æ–‡æ¡£

---

### ç¬¬ä¸‰å±‚ï¼šä¸šåŠ¡åˆ†æ‰¹ç­–ç•¥ï¼ˆè§£å†³å¤šæ–‡æ¡£åœºæ™¯ï¼‰

**ç—‡çŠ¶**: å¤šä¸ªæ–‡æ¡£ï¼ˆ5+ï¼‰ï¼Œæ€»è§„æ¨¡å¤§ï¼Œä¸²è¡Œå¤„ç†æ•ˆç‡ä½

**è§£å†³æ–¹æ¡ˆB: Document Type Batching**ï¼ˆé€‚åˆï¼šå¤šæ–‡æ¡£+å¤šç±»å‹ï¼‰

```python
# 1. æŒ‰ä¸šåŠ¡é€»è¾‘åˆ†ç±»ï¼ˆéæŠ€æœ¯é™åˆ¶ï¼‰
DOC_TYPES = ['è®¾ç«‹ç±»', 'è‚¡æƒè½¬è®©ç±»', 'å¢èµ„ç±»', 'å…¶ä»–å˜æ›´ç±»']

def classify_document(filename, content_preview):
    if 'è‚¡æƒè½¬è®©' in content_preview:
        return 'è‚¡æƒè½¬è®©ç±»'
    # ... å…¶ä»–ç±»å‹åˆ¤æ–­

# 2. æŒ‰ä¸šåŠ¡é¡ºåºåˆ†æ‰¹
batch_order = ['è®¾ç«‹ç±»', 'å¢èµ„ç±»', 'è‚¡æƒè½¬è®©ç±»', 'å…¶ä»–å˜æ›´ç±»']

# 3. åˆ†æ‰¹å¤„ç† + è·¨æ‰¹éªŒè¯
for batch_type in batch_order:
    docs = doc_type_groups[batch_type]
    batch_results = process_batch(docs)
    all_results.extend(batch_results)

# 4. è·¨æ‰¹ä¸€è‡´æ€§éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰
verify_continuity(all_results)  # Event N end = Event N+1 start
```

**å…³é”®ç‚¹**:
- âœ… ç”¨ä¸šåŠ¡é€»è¾‘åˆ†æ‰¹ï¼ˆä¸åªæ˜¯æ–‡ä»¶å¤§å°ï¼‰
- âœ… æ‰¹æ¬¡æœ‰ä¼˜å…ˆçº§ï¼ˆè®¾ç«‹â†’å¢èµ„â†’è½¬è®©â†’å…¶ä»–ï¼‰
- âœ… æ‰¹é—´éªŒè¯ï¼ˆç¡®ä¿è¿ç»­æ€§ï¼‰
- âœ… é‡æ–°æ’åºï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼Œéæ‰¹æ¬¡é¡ºåºï¼‰

**æ•ˆæœ**: æ¸…æ™°çš„ä¸šåŠ¡é€»è¾‘ + æ˜ç¡®çš„è¿›åº¦è¿½è¸ª

---

## ğŸ”„ è·¨é¡¹ç›®å€Ÿé‰´æ–¹æ³•è®º

### å‘ç°å¯å€Ÿé‰´æ–¹æ¡ˆçš„3ä¸ªç»´åº¦

| ç»´åº¦ | é—®é¢˜ | æ£€æŸ¥ç‚¹ |
|------|------|--------|
| **é—®é¢˜åœºæ™¯** | å…¶ä»–é¡¹ç›®é‡åˆ°ç±»ä¼¼é—®é¢˜å—ï¼Ÿ | å¤§æ–‡æ¡£ã€å¤šæ–‡æ¡£ã€å¤æ‚ææ–™ |
| **åˆ†æ‰¹ä¾æ®** | ä»–ä»¬æŒ‰ä»€ä¹ˆé€»è¾‘åˆ†æ‰¹ï¼Ÿ | æŠ€æœ¯é™åˆ¶ vs ä¸šåŠ¡é€»è¾‘ |
| **è§¦å‘æ–¹å¼** | è‡ªåŠ¨ vs æ‰‹åŠ¨ï¼Ÿ | ç”¨æˆ·ä½“éªŒå·®å¼‚ |

### å¯¹æ¯”åˆ†ææ¨¡æ¿

```
æ–¹æ¡ˆAï¼ˆå½“å‰é¡¹ç›®ï¼‰         æ–¹æ¡ˆBï¼ˆå…¶ä»–é¡¹ç›®ï¼‰
â”œâ”€ åˆ†æ‰¹ä¾æ®: _______    vs  _______
â”œâ”€ è§¦å‘æ–¹å¼: _______    vs  _______
â”œâ”€ é€‚ç”¨åœºæ™¯: _______    vs  _______
â””â”€ ä¼˜ç¼ºç‚¹:   _______    vs  _______

èåˆæ€è·¯:
â†’ å–Açš„______ä¼˜åŠ¿ + Bçš„______ä¼˜åŠ¿
â†’ è¦†ç›–åœºæ™¯: Aåœºæ™¯ + Båœºæ™¯ + æ··åˆåœºæ™¯
```

### èåˆå†³ç­–æ ‘

```
æ˜¯å¦éœ€è¦èåˆï¼Ÿ
â”œâ”€ æ˜¯å¦è§£å†³ä¸åŒé—®é¢˜ï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ å¯èƒ½äº’è¡¥ï¼Œç»§ç»­è¯„ä¼°
â”‚   â””â”€ å¦ â†’ é€‰æœ€ä¼˜æ–¹æ¡ˆï¼Œæ— éœ€èåˆ
â”œâ”€ æ˜¯å¦æœ‰å†²çªï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ è®¾è®¡åœºæ™¯åˆ‡æ¢é€»è¾‘
â”‚   â””â”€ å¦ â†’ å¯ä»¥å åŠ ä½¿ç”¨
â””â”€ èåˆåå¤æ‚åº¦ï¼Ÿ
    â”œâ”€ ä½ â†’ å®æ–½èåˆ
    â””â”€ é«˜ â†’ åˆ†åœºæ™¯ç‹¬ç«‹å®ç°
```

---

## ğŸ—ï¸ å®æ–½æ­¥éª¤æ¨¡æ¿

### é˜¶æ®µ1: åœºæ™¯æ£€æµ‹å±‚ï¼ˆ3å°æ—¶ï¼‰

```markdown
## Step X.1: Material Assessment (ææ–™è¯„ä¼°)

Calculate statistics:
- total_docs, total_size_kb, large_doc_count
- doc_type_diversity

## Step X.2: Scenario Detection (åœºæ™¯æ£€æµ‹)

if single large doc:
    scenario = "A"; strategy = "section_chunking"
elif multiple docs + large total:
    scenario = "B"; strategy = "type_batching"
elif many docs:
    scenario = "C"; strategy = "parallel"
else:
    scenario = "D"; strategy = "standard"

print(f"Scenario: {scenario}, Strategy: {strategy}")
```

### é˜¶æ®µ2: ç­–ç•¥å®æ–½å±‚ï¼ˆ6-8å°æ—¶ï¼‰

```markdown
## Scenario A: Section-Based Chunking
[å®ç°æ–‡æ¡£åˆ†æ®µé€»è¾‘]

## Scenario B: Document Type Batching
[å®ç°ä¸šåŠ¡åˆ†æ‰¹é€»è¾‘]

## Scenario C: Parallel Processing
[å®ç°å¹¶è¡Œä¼˜åŒ–]

## Scenario D: Standard Processing
[ä¿æŒç°æœ‰é€»è¾‘]
```

### é˜¶æ®µ3: å…ƒæ•°æ®å±‚ï¼ˆ2å°æ—¶ï¼‰

```json
{
  "processing_metadata": {
    "scenario": "A|B|C|D",
    "strategy_used": "...",
    "material_statistics": {...},
    "batch_processing": {...},
    "performance": {...}
  }
}
```

**æ€»å·¥ä½œé‡**: çº¦12å°æ—¶ï¼ˆå«æµ‹è¯•ï¼‰

---

## âš ï¸ å¸¸è§é™·é˜±

| é™·é˜± | è¡¨ç° | è§£å†³ |
|------|------|------|
| **è¿‡åº¦ä¼˜åŒ–** | ä¸ºå°æ–‡æ¡£ä¹Ÿåˆ†æ®µ | è®¾é˜ˆå€¼ï¼ˆ<50KBæ ‡å‡†è¯»å–ï¼‰ |
| **ä¸¢å¤±ä¸šåŠ¡é€»è¾‘** | åªæŒ‰æ–‡ä»¶å¤§å°åˆ† | å…ˆä¸šåŠ¡åˆ†ç±»ï¼Œå†æŠ€æœ¯ä¼˜åŒ– |
| **ç¼ºå°‘éªŒè¯** | æ‰¹é—´æ•°æ®ä¸è¿ç»­ | å¿…é¡»åšè·¨æ‰¹ä¸€è‡´æ€§éªŒè¯ |
| **ä¸é€æ˜** | ç”¨æˆ·ä¸çŸ¥æ€ä¹ˆå¤„ç†çš„ | è¾“å‡ºprocessing_metadata |
| **ç¡¬ç¼–ç é˜ˆå€¼** | 50KBå›ºå®šæ­» | æ ¹æ®å®é™…æµ‹è¯•è°ƒæ•´ |

---

## âœ… æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

**è®¾è®¡é˜¶æ®µ**:
- [ ] æ˜¯å¦åˆ†æäº†å…¶ä»–é¡¹ç›®çš„ç±»ä¼¼ä¼˜åŒ–ï¼Ÿ
- [ ] æ˜¯å¦åŒºåˆ†äº†"æŠ€æœ¯é™åˆ¶"å’Œ"ä¸šåŠ¡é€»è¾‘"ï¼Ÿ
- [ ] æ˜¯å¦è®¾è®¡äº†åœºæ™¯æ£€æµ‹é€»è¾‘ï¼Ÿ
- [ ] æ˜¯å¦è€ƒè™‘äº†å‘åå…¼å®¹ï¼Ÿ

**å®æ–½é˜¶æ®µ**:
- [ ] æ˜¯å¦ä¿ç•™äº†åŸæœ‰ä¼˜åŠ¿ï¼Ÿ
- [ ] æ˜¯å¦æ·»åŠ äº†å®Œæ•´æ€§æ‰¿è¯ºï¼Ÿ
- [ ] æ˜¯å¦å®ç°äº†è‡ªåŠ¨æ£€æµ‹ï¼ˆéæ‰‹åŠ¨ï¼‰ï¼Ÿ
- [ ] æ˜¯å¦ç”Ÿæˆäº†å¤„ç†å…ƒæ•°æ®ï¼Ÿ

**æµ‹è¯•é˜¶æ®µ**:
- [ ] æ˜¯å¦æµ‹è¯•äº†æ‰€æœ‰åœºæ™¯ï¼ˆA/B/C/Dï¼‰ï¼Ÿ
- [ ] æ˜¯å¦éªŒè¯äº†è·¨æ‰¹ä¸€è‡´æ€§ï¼Ÿ
- [ ] æ˜¯å¦æ£€æŸ¥äº†å…ƒæ•°æ®å®Œæ•´æ€§ï¼Ÿ
- [ ] æ˜¯å¦å¯¹æ¯”äº†ä¼˜åŒ–å‰åæ•ˆæœï¼Ÿ

---

## ğŸ¯ å¿«é€Ÿå†³ç­–å‚è€ƒ

**é‡åˆ°æ–°çš„å¤§æ–‡æ¡£é—®é¢˜æ—¶ï¼ŒæŒ‰æ­¤é¡ºåºæ€è€ƒ**:

1. **é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ**
   - å•æ–‡æ¡£å¤ªå¤§ â†’ Scenario A
   - å¤šæ–‡æ¡£åˆ†ç±»æ··ä¹± â†’ Scenario B
   - æ–‡æ¡£å¤ªå¤šå¤„ç†æ…¢ â†’ Scenario C

2. **æœ‰ç°æˆæ–¹æ¡ˆå—ï¼Ÿ**
   - æ£€æŸ¥å…¶ä»–é¡¹ç›®
   - å¯¹æ¯”ä¼˜åŠ£
   - è€ƒè™‘èåˆ

3. **å®æ–½ä¼˜å…ˆçº§ï¼Ÿ**
   - P0: å¼ºåˆ¶å®Œæ•´æ€§ï¼ˆ2hï¼Œè§£å†³80%é—®é¢˜ï¼‰
   - P1: æ ¸å¿ƒåœºæ™¯ä¼˜åŒ–ï¼ˆ6hï¼‰
   - P2: å…ƒæ•°æ®å’Œç›‘æ§ï¼ˆ2hï¼‰

4. **å¦‚ä½•éªŒè¯ï¼Ÿ**
   - æ‰¾æœ€å¤æ‚çš„çœŸå®æ¡ˆä¾‹æµ‹è¯•
   - æ£€æŸ¥processing_metadata
   - å¯¹æ¯”ä¼˜åŒ–å‰åæ•°æ®

---

## ğŸ“š ä»£ç æ¨¡æ¿åº“

### æ¨¡æ¿1: æ–‡æ¡£å¤§å°æ£€æµ‹

```python
def assess_document_size(file_path):
    size_kb = file_path.stat().st_size / 1024
    estimated_lines = int(size_kb * 17)  # ä¸­æ–‡æ–‡æœ¬ä¼°ç®—

    if size_kb > 80 or estimated_lines > 2000:
        return "large", "chunked_required"
    elif size_kb > 50 or estimated_lines > 1500:
        return "medium", "chunked_optional"
    else:
        return "small", "full_read"
```

### æ¨¡æ¿2: ä¸šåŠ¡ç±»å‹åˆ†ç±»

```python
def classify_by_business_type(filename, content_preview):
    # æ ¹æ®ä¸šåŠ¡å®šä¹‰å…³é”®è¯æ˜ å°„
    TYPE_KEYWORDS = {
        'TypeA': ['keyword1', 'keyword2'],
        'TypeB': ['keyword3', 'keyword4'],
        'TypeC': ['keyword5'],
    }

    for doc_type, keywords in TYPE_KEYWORDS.items():
        if any(kw in content_preview for kw in keywords):
            return doc_type

    return 'Unclassified'
```

### æ¨¡æ¿3: åœºæ™¯æ£€æµ‹

```python
def detect_scenario(total_docs, total_size_kb, large_count, type_count):
    if total_docs == 1 and total_size_kb > 50:
        return "A", "single_large", "section_chunking"
    elif total_docs >= 5 and total_size_kb > 200:
        return "B", "multiple_large", "type_batching"
    elif total_docs >= 15:
        return "C", "many_docs", "parallel"
    else:
        return "D", "normal", "standard"
```

### æ¨¡æ¿4: Processing Metadata

```python
processing_metadata = {
    "scenario": scenario,
    "strategy_used": strategy,
    "material_statistics": {
        "total_documents": total_docs,
        "total_size_kb": round(total_size_kb, 1),
        "large_documents_count": large_count,
    },
    "batch_processing": {
        "enabled": bool(batches),
        "batches": batch_log if batches else []
    },
    "performance": {
        "total_time_seconds": duration,
        "average_per_item": avg_time,
    }
}
```

---

## ğŸ”§ é¡¹ç›®é€‚é…æŒ‡å—

**å°†æœ¬æ–¹æ¡ˆåº”ç”¨åˆ°æ–°é¡¹ç›®çš„5æ­¥æ³•**:

1. **è¯†åˆ«ç›¸ä¼¼æ€§** (30åˆ†é’Ÿ)
   - æ˜¯å¦æœ‰å¤§æ–‡æ¡£å¤„ç†ï¼Ÿ
   - æ˜¯å¦æœ‰å¤šæ–‡æ¡£åœºæ™¯ï¼Ÿ
   - å½“å‰é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

2. **å®šä¹‰ä¸šåŠ¡åˆ†ç±»** (1å°æ—¶)
   - æ ¹æ®ä¸šåŠ¡å®šä¹‰æ–‡æ¡£ç±»å‹
   - ç¡®å®šåˆ†æ‰¹ä¼˜å…ˆçº§é¡ºåº
   - è®¾è®¡å…³é”®è¯æ˜ å°„

3. **è°ƒæ•´é˜ˆå€¼å‚æ•°** (30åˆ†é’Ÿ)
   - æ ¹æ®å®é™…æ–‡æ¡£æµ‹è¯•è°ƒæ•´50KBé˜ˆå€¼
   - æ ¹æ®tokené™åˆ¶è°ƒæ•´åˆ†æ®µå¤§å°
   - æ ¹æ®æ–‡æ¡£æ•°é‡è°ƒæ•´æ‰¹æ¬¡è§¦å‘æ¡ä»¶

4. **å®æ–½æ ¸å¿ƒä»£ç ** (6å°æ—¶)
   - å¤ç”¨æœ¬æ–‡æ¡£çš„ä»£ç æ¨¡æ¿
   - æ›¿æ¢ä¸šåŠ¡ç›¸å…³çš„éƒ¨åˆ†ï¼ˆç±»å‹åç§°ã€å…³é”®è¯ç­‰ï¼‰
   - ä¿æŒæ¡†æ¶ç»“æ„ä¸å˜

5. **æµ‹è¯•å’Œä¼˜åŒ–** (2å°æ—¶)
   - ç”¨çœŸå®å¤æ‚æ¡ˆä¾‹æµ‹è¯•
   - æ£€æŸ¥processing_metadata
   - æ ¹æ®ç»“æœå¾®è°ƒå‚æ•°

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**ç›¸å…³æ–‡æ¡£**:
- `/docs/å…¬å¸å†å²æå–ä¼˜åŒ–å®æ–½æŠ¥å‘Š.md` - å®Œæ•´å®æ–½æ¡ˆä¾‹
- `.claude/agents/corp-history-extractor.md` - å®Œæ•´ä»£ç å®ç°
- å€ºæƒå°½è°ƒé¡¹ç›®çš„Exception 6 - æ‰¹å¤„ç†åŸå‹æ–¹æ¡ˆ

**å…³é”®æ¦‚å¿µ**:
- Tokené™åˆ¶: LLMå•æ¬¡å¤„ç†çš„æ–‡æœ¬é‡ä¸Šé™
- Section-based chunking: åŸºäºæ–‡æ¡£ç»“æ„çš„æ™ºèƒ½åˆ†æ®µ
- Type-based batching: åŸºäºä¸šåŠ¡ç±»å‹çš„æ‰¹å¤„ç†
- Processing metadata: å¤„ç†è¿‡ç¨‹çš„é€æ˜åŒ–è®°å½•

---

**æœ€åæ›´æ–°**: 2025-11-13
**ç»´æŠ¤è€…**: Claude Code Optimization Team
**é€‚ç”¨ç‰ˆæœ¬**: Claude Sonnet 4.5+
